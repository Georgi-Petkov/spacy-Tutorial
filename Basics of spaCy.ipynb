{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASICS OF spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading spacy library\n",
    "import spacy\n",
    "nlp=spacy.load('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We are learning spaCy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1=nlp(\"We are learning spaCy\")\n",
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Guys,\\n\\nWe are learning spaCy , a cool nlp library.\\n\\nSome of its Features are:-\\n\\nEasy deep learning integration.\\nNon-destructive tokenization.\\nExport to numpy data arrays.\\nNamed entity recognition.\\nSupport for 51+ languages.\\nPre-trained word vectors.\\nState-of-the-art speed.\\nPart-of-speech tagging.\\nRobust, rigorously evaluated accuracy and many more\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfile=open(\"learnspaCy.txt\").read()\n",
    "myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello Guys,\n",
       "\n",
       "We are learning spaCy , a cool nlp library.\n",
       "\n",
       "Some of its Features are:-\n",
       "\n",
       "Easy deep learning integration.\n",
       "Non-destructive tokenization.\n",
       "Export to numpy data arrays.\n",
       "Named entity recognition.\n",
       "Support for 51+ languages.\n",
       "Pre-trained word vectors.\n",
       "State-of-the-art speed.\n",
       "Part-of-speech tagging.\n",
       "Robust, rigorously evaluated accuracy and many more"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1=nlp(myfile)\n",
    "file1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:Hello Guys,\n",
      "\n",
      "We are learning spaCy , a cool nlp library.\n",
      "\n",
      "\n",
      "1:Some of its Features are:-\n",
      "\n",
      "Easy deep learning integration.\n",
      "\n",
      "2:Non-destructive tokenization.\n",
      "\n",
      "3:Export to numpy data arrays.\n",
      "\n",
      "4:Named entity recognition.\n",
      "\n",
      "5:Support for 51+ languages.\n",
      "\n",
      "6:Pre-trained word vectors.\n",
      "\n",
      "7:State-of-the-art speed.\n",
      "\n",
      "8:Part-of-speech tagging.\n",
      "\n",
      "9:Robust, rigorously evaluated accuracy and many more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num,sentence in enumerate(file1.sents):\n",
    "    print(f'{num}:{sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "We are learning spaCy"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "are\n",
      "learning\n",
      "spaCy\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'learning', 'spaCy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For getting list of words , use split() method\n",
    "\n",
    "doc1.text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I have 3 coins and a 10 rupee note"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2=nlp(\"I have 3 coins and a 10 rupee note\")\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I True\n",
      "have True\n",
      "3 False\n",
      "coins True\n",
      "and True\n",
      "a True\n",
      "10 False\n",
      "rupee True\n",
      "note True\n"
     ]
    }
   ],
   "source": [
    "## is_alpha property\n",
    "for word in doc2:\n",
    "    print(word.text,word.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I True\n",
      "have True\n",
      "3 False\n",
      "coins False\n",
      "and True\n",
      "a True\n",
      "10 False\n",
      "rupee False\n",
      "note False\n"
     ]
    }
   ],
   "source": [
    "## is_stop property\n",
    "for word in doc2:\n",
    "    print(word.text,word.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Xx\n",
      "are xxx\n",
      "learning xxxx\n",
      "spaCy xxxXx\n"
     ]
    }
   ],
   "source": [
    "## shape property\n",
    "for word in doc1:\n",
    "    print(word.text,word.shape_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRON\n",
      "are VERB\n",
      "learning VERB\n",
      "spaCy NOUN\n"
     ]
    }
   ],
   "source": [
    "## .pos_ property\n",
    "for word in doc1:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We PRON PRP\n",
      "are VERB VBP\n",
      "learning VERB VBG\n",
      "spaCy NOUN NN\n"
     ]
    }
   ],
   "source": [
    "## .tag_ property\n",
    "for word in doc1:\n",
    "    print(word.text,word.pos_,word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, singular or mass'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## meaning of pos abbrev.\n",
    "spacy.explain('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'verb, non-3rd person singular present'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('VBP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual dependency using displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c13aaeeec0464f08b3f6664accd331a6-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">We</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">spaCy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c13aaeeec0464f08b3f6664accd331a6-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c13aaeeec0464f08b3f6664accd331a6-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c13aaeeec0464f08b3f6664accd331a6-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c13aaeeec0464f08b3f6664accd331a6-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c13aaeeec0464f08b3f6664accd331a6-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c13aaeeec0464f08b3f6664accd331a6-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc1,style='dep',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=nlp(\"playing played player\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing play\n",
      "played play\n",
      "player player\n"
     ]
    }
   ],
   "source": [
    "for word in doc3:\n",
    "    print(word.text,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4=nlp(\"walks walk walked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walks walk NOUN\n",
      "walk walk VERB\n",
      "walked walk VERB\n"
     ]
    }
   ],
   "source": [
    "for word in doc4:\n",
    "    print(word.text,word.lemma_,word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition or Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5=nlp(\"By 2025 , India will grow so much in Technical field and earn more than 5 million dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 DATE\n",
      "India GPE\n",
      "Technical GPE\n",
      "more than 5 million dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "for word in doc5.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">By \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2025\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " will grow so much in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Technical\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " field and earn \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    more than 5 million dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc5,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1=nlp(\"dog\")\n",
    "word2=nlp(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7952184229586672"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## similarity between words\n",
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5=nlp(\"cat dog bird fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat', 'cat') Similarly :- 1.0\n",
      "('cat', 'dog') Similarly :- 0.6008464\n",
      "('cat', 'bird') Similarly :- 0.62692624\n",
      "('cat', 'fish') Similarly :- 0.33569032\n",
      "('dog', 'cat') Similarly :- 0.6008464\n",
      "('dog', 'dog') Similarly :- 1.0\n",
      "('dog', 'bird') Similarly :- 0.7429262\n",
      "('dog', 'fish') Similarly :- 0.2662138\n",
      "('bird', 'cat') Similarly :- 0.62692624\n",
      "('bird', 'dog') Similarly :- 0.7429262\n",
      "('bird', 'bird') Similarly :- 1.0\n",
      "('bird', 'fish') Similarly :- 0.39694816\n",
      "('fish', 'cat') Similarly :- 0.33569032\n",
      "('fish', 'dog') Similarly :- 0.2662138\n",
      "('fish', 'bird') Similarly :- 0.39694816\n",
      "('fish', 'fish') Similarly :- 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/home/anu/documents/software/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "## similarity between words in a sentence\n",
    "\n",
    "for w1 in doc5:\n",
    "    for w2 in doc5:\n",
    "        print((w1.text,w2.text),\"Similarly :-\",w1.similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'please', 'sometime', 'whose', 'now', 'nevertheless', 'throughout', 'enough', 'his', 'made', 'they', 'there', 'and', 'always', 'did', 'somehow', 'every', 'through', 'go', 'thereby', 'yours', 'others', 'except', 'so', 'your', 'wherever', 'across', 'between', 'under', 'within', 'sometimes', 'must', '‘ll', 'everywhere', 'mine', 'towards', 'really', 'being', 'once', 'if', 'those', 'although', 'thereupon', 'thereafter', 'name', 'via', 'each', 'make', 'been', 'nor', 'this', 'two', 'would', 'am', 'herein', 'yourself', 'them', 'nobody', 'one', 'further', 'perhaps', 'as', 'below', 'though', 'quite', 'other', 'nothing', 'hundred', 'alone', 'whoever', 'nine', 'therefore', 'elsewhere', 'besides', 'whole', 'during', 'along', 'much', 're', '‘re', 'hereby', 'together', 'per', 'seeming', 'whenever', 'doing', 'front', \"'ll\", 'formerly', \"'s\", 'anyone', 'meanwhile', 'up', 'itself', 'very', '‘m', 'third', 'six', 'after', 'almost', 'sixty', 'me', 'latterly', 'to', 'while', 'himself', 'whom', 'side', 'whereafter', 'give', 'everyone', 'fifty', 'part', 'in', '‘d', 'ourselves', \"'m\", 'not', 'but', 'top', 'here', \"n't\", 'might', 'various', 'above', 'neither', 'myself', 'upon', 'her', 'onto', 'it', 'five', 'wherein', 'seemed', 'everything', 'unless', 'something', 'another', 'few', 'thence', 'well', 'anywhere', 'either', 'from', 'beyond', 'former', 'rather', 'have', 'yourselves', 'any', 'anything', 'again', 'empty', 'our', 'on', 'why', 'whence', 'n’t', 'themselves', 'eight', 'could', 'three', 'with', 'already', 'cannot', 'too', 'whereby', 'whatever', '’d', 'whereas', 'because', 'becomes', 'its', 'when', 'was', 'may', 'should', 'done', 'never', 'seem', 'own', 'same', 'hence', 'nowhere', 'just', 'hers', 'such', 'who', 'next', 'first', 'many', 'i', 'by', 'my', 'you', 'were', 'where', 'else', 'someone', 'without', 'amount', 'herself', 'yet', 'moreover', 'their', 'will', '’s', '‘s', 'no', \"'re\", 'us', 'due', 'hereupon', '’ll', 'since', 'move', 'of', 'regarding', 'full', 'a', '’re', 'be', 'until', 'four', 'hereafter', 'ever', 'had', 'she', 'anyhow', 'beforehand', 'behind', '’m', 'into', 'that', 'an', 'among', 'afterwards', 'amongst', 'seems', 'ten', 'around', 'only', 'does', 'mostly', 'against', 'all', 'more', 'therein', 'get', 'eleven', 'several', '‘ve', 'using', 'still', 'becoming', 'take', 'which', 'how', 'became', 'most', 'off', 'are', 'ca', 'than', 'whether', 'whereupon', '’ve', 'down', 'before', 'fifteen', 'twelve', 'about', 'n‘t', 'used', 'less', 'we', 'at', 'back', 'over', 'what', 'call', 'is', 'twenty', 'him', 'none', 'some', 'the', 'bottom', 'say', 'see', 'namely', 'out', 'last', 'often', 'show', 'whither', 'indeed', 'for', 'noone', 'put', 'then', 'also', 'ours', 'become', \"'ve\", 'thru', 'do', 'otherwise', 'both', 'even', 'least', 'somewhere', 'toward', \"'d\", 'can', 'anyway', 'forty', 'has', 'thus', 'latter', 'keep', 'these', 'he', 'serious', 'however', 'or', 'beside'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS.add('ohh')\n",
    "nlp.vocab[\"ohh\"].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc15=nlp('the man playing football is a great player.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man\n",
      "football\n",
      "a great player\n"
     ]
    }
   ],
   "source": [
    "for w in doc15.noun_chunks:\n",
    "    print(w.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man\n",
      "football\n",
      "player\n"
     ]
    }
   ],
   "source": [
    "## get root words\n",
    "for w in doc15.noun_chunks:\n",
    "    print(w.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man --connected by = is\n",
      "football --connected by = playing\n",
      "player --connected by = is\n"
     ]
    }
   ],
   "source": [
    "for w in doc15.noun_chunks:\n",
    "    print(w.root.text,\"--connected by =\",w.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation and Boundary Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc25=nlp(\"Hello friends ,we are learning spaCy. Are you all enjoying? keep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello friends ,we are learning spaCy.\n",
      "Are you all enjoying?\n",
      "keep learning\n"
     ]
    }
   ],
   "source": [
    "for sent in doc25.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=nlp(\"spaCy is an amazing library\\nWe want to learn it in depth\\nThat's why we are here :P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is an amazing library\n",
      "We want to learn it in depth\n",
      "\n",
      "That's why we are here :P\n"
     ]
    }
   ],
   "source": [
    "for s in doc3.sents:\n",
    "    print(s.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
